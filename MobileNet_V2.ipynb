{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MobileNet_V2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuMcnmvYL9Mb",
        "colab_type": "code",
        "outputId": "9eb8b0ee-8cdd-4bcd-c3a1-d420791bcd69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "################################################################################\n",
        "#\n",
        "# xNNs_Code_030_CIFAR_ResNetV2.py\n",
        "#\n",
        "# DESCRIPTION\n",
        "#\n",
        "#    TensorFlow image classification using CIFAR\n",
        "#\n",
        "# INSTRUCTIONS\n",
        "#\n",
        "#    1. Go to Google Colaboratory: https://colab.research.google.com/notebooks/welcome.ipynb\n",
        "#    2. File - New Python 3 notebook\n",
        "#    3. Cut and paste this file into the cell (feel free to divide into multiple cells)\n",
        "#    4. Runtime - Change runtime type - Hardware accelerator - GPU\n",
        "#    5. Runtime - Run all\n",
        "#\n",
        "# NOTES\n",
        "#\n",
        "#    1. This configuration achieves 91.6% accuracy in 60 epochs with each epoch\n",
        "#       taking ~ 135s on Google Colab.  Accuracy can be improved via\n",
        "#       - Improved training data augmentation\n",
        "#       - Improved network design\n",
        "#       - Improved network training\n",
        "#\n",
        "#    2. Examples (currently commented out) are included for the following\n",
        "#       - Computing the dataset mean and std dev\n",
        "#       - Restarting training after a crash from the last saved checkpoint\n",
        "#       - Saving and loading the model in Keras H5 format\n",
        "#       - Saving and loading the model in TensorFlow SavedModel format\n",
        "#       - Getting a list of all feature maps\n",
        "#       - Creating an encoder only model\n",
        "#\n",
        "################################################################################\n",
        "\n",
        "################################################################################\n",
        "#\n",
        "# IMPORT\n",
        "#\n",
        "################################################################################\n",
        "\n",
        "# tensorflow 2.0 beta and tensorflow datasets\n",
        "!pip install tensorflow-gpu==2.0.0-beta1\n",
        "!pip install tensorflow-datasets\n",
        "\n",
        "# tenorflow\n",
        "import tensorflow as     tf\n",
        "from   tensorflow import keras\n",
        "\n",
        "# tensorflow datasets\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# additional libraries\n",
        "import math\n",
        "import numpy             as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "################################################################################\n",
        "#\n",
        "# PARAMETERS\n",
        "#\n",
        "################################################################################\n",
        "\n",
        "# data\n",
        "DATA_NUM_CLASSES        = 10\n",
        "DATA_CHANNELS           = 3\n",
        "DATA_ROWS               = 32\n",
        "DATA_COLS               = 32\n",
        "DATA_CROP_ROWS          = 28\n",
        "DATA_CROP_COLS          = 28\n",
        "DATA_MEAN               = np.array([[[125.30691805, 122.95039414, 113.86538318]]]) # CIFAR10\n",
        "DATA_STD_DEV            = np.array([[[ 62.99321928,  62.08870764,  66.70489964]]]) # CIFAR10\n",
        "\n",
        "# model\n",
        "MODEL_LEVEL_0_BLOCKS    = 4\n",
        "MODEL_LEVEL_1_BLOCKS    = 6\n",
        "MODEL_LEVEL_2_BLOCKS    = 3\n",
        "\n",
        "# training\n",
        "TRAINING_BATCH_SIZE      = 32\n",
        "TRAINING_SHUFFLE_BUFFER  = 5000\n",
        "TRAINING_BN_MOMENTUM     = 0.99\n",
        "TRAINING_BN_EPSILON      = 0.001\n",
        "TRAINING_LR_MAX          = 0.001\n",
        "# TRAINING_LR_SCALE        = 0.1\n",
        "# TRAINING_LR_EPOCHS       = 2\n",
        "TRAINING_LR_INIT_SCALE   = 0.01\n",
        "TRAINING_LR_INIT_EPOCHS  = 5\n",
        "TRAINING_LR_FINAL_SCALE  = 0.01\n",
        "TRAINING_LR_FINAL_EPOCHS = 55\n",
        "\n",
        "# training (derived)\n",
        "TRAINING_NUM_EPOCHS = TRAINING_LR_INIT_EPOCHS + TRAINING_LR_FINAL_EPOCHS\n",
        "TRAINING_LR_INIT    = TRAINING_LR_MAX*TRAINING_LR_INIT_SCALE\n",
        "TRAINING_LR_FINAL   = TRAINING_LR_MAX*TRAINING_LR_FINAL_SCALE\n",
        "\n",
        "# saving\n",
        "SAVE_MODEL_PATH = './save/model/'\n",
        "!mkdir -p \"$SAVE_MODEL_PATH\"\n",
        "\n",
        "################################################################################\n",
        "#\n",
        "# DATA\n",
        "#\n",
        "################################################################################\n",
        "\n",
        "# pre processing for training data\n",
        "def pre_processing_train(example):\n",
        "\n",
        "    # extract image and label from example\n",
        "    image = example[\"image\"]\n",
        "    label = example[\"label\"]\n",
        "  \n",
        "    # image is cast to float32, normalized, augmented and random cropped\n",
        "    # label is cast to int32\n",
        "    image = tf.math.divide(tf.math.subtract(tf.dtypes.cast(image, tf.float32), DATA_MEAN), DATA_STD_DEV)\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_crop(image, size=[DATA_CROP_ROWS, DATA_CROP_COLS, 3])\n",
        "    label = tf.dtypes.cast(label, tf.int32)\n",
        "    \n",
        "    # return image and label\n",
        "    return image, label\n",
        "\n",
        "# pre processing for testing data\n",
        "def pre_processing_test(example):\n",
        "\n",
        "    # extract image and label from example\n",
        "    image = example[\"image\"]\n",
        "    label = example[\"label\"]\n",
        "\n",
        "    # image is cast to float32, normalized, augmented and center cropped\n",
        "    # label is cast to int32\n",
        "    image = tf.math.divide(tf.math.subtract(tf.dtypes.cast(image, tf.float32), DATA_MEAN), DATA_STD_DEV)\n",
        "    image = tf.image.crop_to_bounding_box(image, (DATA_ROWS - DATA_CROP_ROWS) // 2, (DATA_COLS - DATA_CROP_COLS) // 2, DATA_CROP_ROWS, DATA_CROP_COLS)\n",
        "    label = tf.dtypes.cast(label, tf.int32)\n",
        "    \n",
        "    # return image and label\n",
        "    return image, label\n",
        "\n",
        "# download data and split into training and testing datasets\n",
        "dataset_train, info = tfds.load(\"cifar10\", split=tfds.Split.TRAIN, with_info=True)\n",
        "dataset_test,  info = tfds.load(\"cifar10\", split=tfds.Split.TEST,  with_info=True)\n",
        "\n",
        "# debug - datasets\n",
        "# print(dataset_train) # <_OptionsDataset shapes: {image: (32, 32, 3), label: ()}, types: {image: tf.uint8, label: tf.int64}>\n",
        "# print(dataset_test)  # <_OptionsDataset shapes: {image: (32, 32, 3), label: ()}, types: {image: tf.uint8, label: tf.int64}>\n",
        "\n",
        "# training data mean\n",
        "# num_elem  = 0.0\n",
        "# data_mean = np.array([0.0, 0.0, 0.0])\n",
        "# for elem in dataset_train:\n",
        "#     z         = np.copy(tf.dtypes.cast(elem[\"image\"], tf.float32))\n",
        "#     data_mean = data_mean + np.mean(z, axis=(0, 1))\n",
        "#     num_elem  = num_elem + 1.0\n",
        "# data_mean = data_mean/num_elem\n",
        "# data_mean = data_mean.reshape(1, 1, 3)\n",
        "# print(data_mean)\n",
        "\n",
        "# training data std dev\n",
        "# num_elem = 0.0\n",
        "# data_std = np.array([0.0, 0.0, 0.0])\n",
        "# for elem in dataset_train:\n",
        "#     z        = np.copy(tf.dtypes.cast(elem[\"image\"], tf.float32))\n",
        "#     data_std = data_std + np.sum((z - data_mean)*(z - data_mean), axis=(0, 1))/float(DATA_ROWS*DATA_COLS)\n",
        "#     num_elem = num_elem + 1.0\n",
        "# data_std = data_std/num_elem\n",
        "# data_std = np.sqrt(data_std)\n",
        "# data_std = data_std.reshape(1, 1, 3)\n",
        "# print(data_std)\n",
        "\n",
        "# transform training dataset\n",
        "dataset_train = dataset_train.map(pre_processing_train, num_parallel_calls=4)\n",
        "dataset_train = dataset_train.shuffle(buffer_size=TRAINING_SHUFFLE_BUFFER)\n",
        "dataset_train = dataset_train.batch(TRAINING_BATCH_SIZE)\n",
        "dataset_train = dataset_train.prefetch(buffer_size=1)\n",
        "\n",
        "# transform testing dataset\n",
        "dataset_test = dataset_test.map(pre_processing_test, num_parallel_calls=4)\n",
        "dataset_test = dataset_test.batch(TRAINING_BATCH_SIZE)\n",
        "dataset_test = dataset_test.prefetch(buffer_size=1)\n",
        "\n",
        "# debug - datasets after transformation\n",
        "# print(dataset_train) # <PrefetchDataset shapes: ((None, 28, 28, 3), (None,)), types: (tf.float32, tf.int32)>\n",
        "# print(dataset_test)  # <PrefetchDataset shapes: ((None, 28, 28, 3), (None,)), types: (tf.float32, tf.int32)>\n",
        "\n",
        "################################################################################\n",
        "#\n",
        "# MODEL\n",
        "#\n",
        "################################################################################\n",
        "\n",
        "# create and compile model\n",
        "def create_model(rows, cols, channels, level_0_blocks, level_1_blocks, level_2_blocks, num_classes, lr_initial):\n",
        "\n",
        "    # encoder - input\n",
        "    model_input = keras.Input(shape=(rows, cols, channels), name='input_image')\n",
        "    x           = model_input\n",
        "    \n",
        "    # encoder - tail\n",
        "    x = keras.layers.Conv2D(32, 3, strides=1, padding='same', activation=None, use_bias=False)(x)\n",
        "    x = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(x)\n",
        "    x = keras.layers.ReLU()(x)\n",
        "    \n",
        "    # output is 28x28x32 which now foes intp bottleneck1\n",
        "    \n",
        "    \n",
        "    residual = keras.layers.Conv2D(32, 1, strides=1, padding='same', activation=None, use_bias=False)(x)\n",
        "    residual = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual)\n",
        "    residual = keras.layers.ReLU()(residual)\n",
        "    \n",
        "    residual = keras.layers.DepthwiseConv2D(3, strides=(1,1), padding='same', activation=None, use_bias=False)(residual)\n",
        "    residual = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual)\n",
        "    residual = keras.layers.ReLU()(residual)\n",
        "    \n",
        "    residual = keras.layers.Conv2D(32, 1, strides=1, padding='same', activation=None, use_bias=False)(x)\n",
        "    residual = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual)\n",
        "    residual = keras.layers.ReLU()(residual)\n",
        "    \n",
        "    x        = keras.layers.Add()([x, residual])\n",
        "    \n",
        "    # output is 28x28x32 \n",
        "    \n",
        "    # to increase chanel size to 64\n",
        "    x = keras.layers.Conv2D(64, 1, strides=2, padding='same', activation=None, use_bias=False)(x)\n",
        "    x = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(x)\n",
        "    x = keras.layers.ReLU()(x)\n",
        "    \n",
        "    #output is 14x14x64\n",
        "    \n",
        "    \n",
        "    # Bottleneck layer1\n",
        "    \n",
        "    for n0 in range(4):\n",
        "      residual = keras.layers.Conv2D(64*6, 1, strides=1, padding='same', activation=None, use_bias=False)(x)\n",
        "      residual = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual)\n",
        "      residual = keras.layers.ReLU()(residual)\n",
        "    \n",
        "      residual = keras.layers.DepthwiseConv2D(3, strides=(1,1), padding='same', activation=None, use_bias=False)(residual)\n",
        "      residual = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual)\n",
        "      residual = keras.layers.ReLU()(residual)\n",
        "    \n",
        "      residual = keras.layers.Conv2D(64, 1, strides=1, padding='same', activation=None, use_bias=False)(x)\n",
        "      residual = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual)\n",
        "      residual = keras.layers.ReLU()(residual)\n",
        "    \n",
        "      x        = keras.layers.Add()([x, residual])\n",
        "      \n",
        "    # output becomes 14x14x64\n",
        "    \n",
        "    x = keras.layers.Conv2D(96, 1, strides=1, padding='same', activation=None, use_bias=False)(x)\n",
        "    x = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(x)\n",
        "    x = keras.layers.ReLU()(x)\n",
        "    \n",
        "    for n1 in range(3):\n",
        "      residual = keras.layers.Conv2D(96*6, 1, strides=1, padding='same', activation=None, use_bias=False)(x)\n",
        "      residual = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual)\n",
        "      residual = keras.layers.ReLU()(residual)\n",
        "    \n",
        "      residual = keras.layers.DepthwiseConv2D(3, strides=(1,1), padding='same', activation=None, use_bias=False)(residual)\n",
        "      residual = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual)\n",
        "      residual = keras.layers.ReLU()(residual)\n",
        "    \n",
        "      residual = keras.layers.Conv2D(96, 1, strides=1, padding='same', activation=None, use_bias=False)(x)\n",
        "      residual = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual)\n",
        "      residual = keras.layers.ReLU()(residual)\n",
        "    \n",
        "      x        = keras.layers.Add()([x, residual])\n",
        "      \n",
        "      \n",
        "    #output is 14x14x96\n",
        "    \n",
        "    x = keras.layers.Conv2D(160, 1, strides=2, padding='same', activation=None, use_bias=False)(x)\n",
        "    x = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(x)\n",
        "    x = keras.layers.ReLU()(x)\n",
        "    \n",
        "    for n2 in range(3):\n",
        "      residual = keras.layers.Conv2D(160*6, 1, strides=1, padding='same', activation=None, use_bias=False)(x)\n",
        "      residual = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual)\n",
        "      residual = keras.layers.ReLU()(residual)\n",
        "    \n",
        "      residual = keras.layers.DepthwiseConv2D(3, strides=(1,1), padding='same', activation=None, use_bias=False)(residual)\n",
        "      residual = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual)\n",
        "      residual = keras.layers.ReLU()(residual)\n",
        "    \n",
        "      residual = keras.layers.Conv2D(160, 1, strides=1, padding='same', activation=None, use_bias=False)(x)\n",
        "      residual = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual)\n",
        "      residual = keras.layers.ReLU()(residual)\n",
        "    \n",
        "      x        = keras.layers.Add()([x, residual])\n",
        "      \n",
        "    # output is 7x7x160\n",
        "    \n",
        "    x = keras.layers.Conv2D(320, 1, strides=1, padding='same', activation=None, use_bias=False)(x)\n",
        "    x = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(x)\n",
        "    x = keras.layers.ReLU()(x)\n",
        "    \n",
        "    residual = keras.layers.Conv2D(320*6, 1, strides=1, padding='same', activation=None, use_bias=False)(x)\n",
        "    residual = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual)\n",
        "    residual = keras.layers.ReLU()(residual)\n",
        "    \n",
        "    residual = keras.layers.DepthwiseConv2D(3, strides=(1,1), padding='same', activation=None, use_bias=False)(residual)\n",
        "    residual = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual)\n",
        "    residual = keras.layers.ReLU()(residual)\n",
        "    \n",
        "    residual = keras.layers.Conv2D(320, 1, strides=1, padding='same', activation=None, use_bias=False)(x)\n",
        "    residual = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(residual)\n",
        "    residual = keras.layers.ReLU()(residual)\n",
        "    \n",
        "    x        = keras.layers.Add()([x, residual])\n",
        "    \n",
        "    \n",
        "    # output 7x7x320\n",
        "    \n",
        "    x = keras.layers.Conv2D(1280, 1, strides=1, padding='same', activation=None, use_bias=False)(x)\n",
        "    x = keras.layers.BatchNormalization(axis=-1, momentum=TRAINING_BN_MOMENTUM, epsilon=TRAINING_BN_EPSILON, center=True, scale=True)(x)\n",
        "    x = keras.layers.ReLU()(x)\n",
        "    \n",
        "    \n",
        "    # encoder - output\n",
        "    encoder_output = x\n",
        "\n",
        "    # decoder\n",
        "    y              = keras.layers.GlobalAveragePooling2D()(encoder_output)\n",
        "    decoder_output = keras.layers.Dense(num_classes, activation='softmax')(y)\n",
        "\n",
        "    # forward path\n",
        "    model = keras.Model(inputs=model_input, outputs=decoder_output, name='resnetv2_model')\n",
        "\n",
        "    # loss, backward path (implicit) and weight update\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr_initial), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # return model\n",
        "    return model\n",
        "\n",
        "# create and compile model\n",
        "model = create_model(DATA_CROP_ROWS, DATA_CROP_COLS, DATA_CHANNELS, MODEL_LEVEL_0_BLOCKS, MODEL_LEVEL_1_BLOCKS, MODEL_LEVEL_2_BLOCKS, DATA_NUM_CLASSES, TRAINING_LR_MAX)\n",
        "\n",
        "# model description and figure\n",
        "model.summary()\n",
        "keras.utils.plot_model(model, 'cifar_model.png', show_shapes=True)\n",
        "\n",
        "################################################################################\n",
        "#\n",
        "# TRAIN AND VALIDATE\n",
        "#\n",
        "################################################################################\n",
        "\n",
        "# learning rate schedule\n",
        "def lr_schedule(epoch):\n",
        "\n",
        "    # staircase\n",
        "    # lr = TRAINING_LR_MAX*math.pow(TRAINING_LR_SCALE, math.floor(epoch/TRAINING_LR_EPOCHS))\n",
        "\n",
        "    # linear warmup followed by cosine decay\n",
        "    if epoch < TRAINING_LR_INIT_EPOCHS:\n",
        "        lr = (TRAINING_LR_MAX - TRAINING_LR_INIT)*(float(epoch)/TRAINING_LR_INIT_EPOCHS) + TRAINING_LR_INIT\n",
        "    else:\n",
        "        lr = (TRAINING_LR_MAX - TRAINING_LR_FINAL)*max(0.0, math.cos(((float(epoch) - TRAINING_LR_INIT_EPOCHS)/(TRAINING_LR_FINAL_EPOCHS - 1.0))*(math.pi/2.0))) + TRAINING_LR_FINAL\n",
        "\n",
        "    # debug - learning rate display\n",
        "    # print(epoch)\n",
        "    # print(lr)\n",
        "\n",
        "    return lr\n",
        "\n",
        "# plot training accuracy and loss curves\n",
        "def plot_training_curves(history):\n",
        "\n",
        "    # training and validation data accuracy\n",
        "    acc     = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "\n",
        "    # training and validation data loss\n",
        "    loss     = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    # plot accuracy\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(acc, label='Training Accuracy')\n",
        "    plt.plot(val_acc, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim([min(plt.ylim()), 1])\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    # plot loss\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(loss, label='Training Loss')\n",
        "    plt.plot(val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.ylabel('Cross Entropy')\n",
        "    plt.ylim([0, 2.0])\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.show()\n",
        "\n",
        "# callbacks (learning rate schedule, model checkpointing during training)\n",
        "callbacks = [keras.callbacks.LearningRateScheduler(lr_schedule),\n",
        "             keras.callbacks.ModelCheckpoint(filepath=SAVE_MODEL_PATH+'model_{epoch}.h5', save_best_only=True, monitor='val_loss', verbose=1)]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.0.0-beta1 in /usr/local/lib/python3.6/dist-packages (2.0.0b1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.2.2)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.14.0.dev2019060501)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.1.7)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.8.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.12.0)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190604,>=1.14.0a20190603 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.14.0a20190603)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.16.5)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.33.6)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.0.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.11.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.15.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta1) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta1) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta1) (41.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-beta1) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (1.16.5)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (19.2.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (2.21.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (2.2.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (4.28.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (1.1.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (0.3.1.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (0.8.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (0.14.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (5.4.8)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (1.11.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (3.7.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets) (0.16.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2019.9.11)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.8)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-datasets) (41.2.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"resnetv2_model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_image (InputLayer)        [(None, 28, 28, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 28, 28, 32)   864         input_image[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 28, 28, 32)   128         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_42 (ReLU)                 (None, 28, 28, 32)   0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 28, 28, 32)   1024        re_lu_42[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 28, 28, 32)   128         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_45 (ReLU)                 (None, 28, 28, 32)   0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 28, 28, 32)   0           re_lu_42[0][0]                   \n",
            "                                                                 re_lu_45[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 14, 14, 64)   2048        add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 14, 14, 64)   256         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_46 (ReLU)                 (None, 14, 14, 64)   0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 14, 14, 64)   4096        re_lu_46[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 14, 14, 64)   256         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_49 (ReLU)                 (None, 14, 14, 64)   0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 14, 14, 64)   0           re_lu_46[0][0]                   \n",
            "                                                                 re_lu_49[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 14, 14, 64)   4096        add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 14, 14, 64)   256         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_52 (ReLU)                 (None, 14, 14, 64)   0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 14, 14, 64)   0           add_13[0][0]                     \n",
            "                                                                 re_lu_52[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 14, 14, 64)   4096        add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 14, 14, 64)   256         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_55 (ReLU)                 (None, 14, 14, 64)   0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 14, 14, 64)   0           add_14[0][0]                     \n",
            "                                                                 re_lu_55[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 14, 14, 64)   4096        add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 14, 14, 64)   256         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_58 (ReLU)                 (None, 14, 14, 64)   0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 14, 14, 64)   0           add_15[0][0]                     \n",
            "                                                                 re_lu_58[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 14, 14, 96)   6144        add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 14, 14, 96)   384         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_59 (ReLU)                 (None, 14, 14, 96)   0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 14, 14, 96)   9216        re_lu_59[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 14, 14, 96)   384         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_62 (ReLU)                 (None, 14, 14, 96)   0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 14, 14, 96)   0           re_lu_59[0][0]                   \n",
            "                                                                 re_lu_62[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 14, 14, 96)   9216        add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 14, 14, 96)   384         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_65 (ReLU)                 (None, 14, 14, 96)   0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 14, 14, 96)   0           add_17[0][0]                     \n",
            "                                                                 re_lu_65[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 14, 14, 96)   9216        add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 14, 14, 96)   384         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_68 (ReLU)                 (None, 14, 14, 96)   0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 14, 14, 96)   0           add_18[0][0]                     \n",
            "                                                                 re_lu_68[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 7, 7, 160)    15360       add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 7, 7, 160)    640         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_69 (ReLU)                 (None, 7, 7, 160)    0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 7, 7, 160)    25600       re_lu_69[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 7, 7, 160)    640         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_72 (ReLU)                 (None, 7, 7, 160)    0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 7, 7, 160)    0           re_lu_69[0][0]                   \n",
            "                                                                 re_lu_72[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 7, 7, 160)    25600       add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 7, 7, 160)    640         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_75 (ReLU)                 (None, 7, 7, 160)    0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 7, 7, 160)    0           add_20[0][0]                     \n",
            "                                                                 re_lu_75[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 7, 7, 160)    25600       add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 7, 7, 160)    640         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_78 (ReLU)                 (None, 7, 7, 160)    0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 7, 7, 160)    0           add_21[0][0]                     \n",
            "                                                                 re_lu_78[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 7, 7, 320)    51200       add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 7, 7, 320)    1280        conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_79 (ReLU)                 (None, 7, 7, 320)    0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 7, 7, 320)    102400      re_lu_79[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 7, 7, 320)    1280        conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_82 (ReLU)                 (None, 7, 7, 320)    0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_23 (Add)                    (None, 7, 7, 320)    0           re_lu_79[0][0]                   \n",
            "                                                                 re_lu_82[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 7, 7, 1280)   409600      add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 7, 7, 1280)   5120        conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_83 (ReLU)                 (None, 7, 7, 1280)   0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 1280)         0           re_lu_83[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           12810       global_average_pooling2d_1[0][0] \n",
            "==================================================================================================\n",
            "Total params: 735,594\n",
            "Trainable params: 728,938\n",
            "Non-trainable params: 6,656\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEPlqmySXFh_",
        "colab_type": "code",
        "outputId": "70dd233f-e3f8-47d6-c4e0-2ddc11345159",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "\n",
        "# training\n",
        "initial_epoch_num = 0\n",
        "history           = model.fit(x=dataset_train, epochs=TRAINING_NUM_EPOCHS, verbose=1, callbacks=callbacks, validation_data=dataset_test, initial_epoch=initial_epoch_num)\n",
        "\n",
        "# example of restarting training after a crash from the last saved checkpoint\n",
        "# model             = create_model(MODEL_LEVEL_0_REPEATS, MODEL_LEVEL_1_REPEATS, MODEL_LEVEL_2_REPEATS)\n",
        "# model.load_weights(SAVE_MODEL_PATH+'model_X.h5') # replace X with the last saved checkpoint number\n",
        "# initial_epoch_num = X                            # replace X with the last saved checkpoint number\n",
        "# history           = model.fit(x=dataset_train, epochs=TRAINING_NUM_EPOCHS, verbose=1, callbacks=callbacks, validation_data=dataset_test, initial_epoch=initial_epoch_num)\n",
        "\n",
        "# plot accuracy and loss curves\n",
        "plot_training_curves(history)\n",
        "\n",
        "################################################################################\n",
        "#\n",
        "# TEST\n",
        "#\n",
        "################################################################################\n",
        "\n",
        "# test\n",
        "test_loss, test_accuracy = model.evaluate(x=dataset_test)\n",
        "print('Test loss:     ', test_loss)\n",
        "print('Test accuracy: ', test_accuracy)\n",
        "\n",
        "# example of saving and loading the model in Keras H5 format\n",
        "# this saves both the model and the weights\n",
        "# model.save('./save/model/model.h5')\n",
        "# new_model       = keras.models.load_model('./save/model/model.h5')\n",
        "# predictions     = model.predict(x=dataset_test)\n",
        "# new_predictions = new_model.predict(x=dataset_test)\n",
        "# np.testing.assert_allclose(predictions, new_predictions, atol=1e-6)\n",
        "\n",
        "# example of saving and loading the model in TensorFlow SavedModel format\n",
        "# this saves both the model and the weights\n",
        "# keras.experimental.export_saved_model(model, './save/model/')\n",
        "# new_model       = keras.experimental.load_from_saved_model('./save/model/')\n",
        "# predictions     = model.predict(x=dataset_test)\n",
        "# new_predictions = new_model.predict(x=dataset_test)\n",
        "# np.testing.assert_allclose(predictions, new_predictions, atol=1e-6)\n",
        "\n",
        "# example of getting a list of all feature maps\n",
        "# feature_map_list = [layer.output for layer in model.layers]\n",
        "# print(feature_map_list)\n",
        "\n",
        "# example of creating a model encoder\n",
        "# replace X with the layer number of the encoder output\n",
        "# model_encoder    = keras.Model(inputs=model.input, outputs=model.layers[X].output)\n",
        "# model_encoder.summary()\n",
        "\n",
        "################################################################################\n",
        "#\n",
        "# DISPLAY\n",
        "#\n",
        "################################################################################\n",
        "\n",
        "# extract a batch from the testing dataset\n",
        "# then extract images and labels for this batch\n",
        "dataset_display                = dataset_test.take(1)\n",
        "it                             = iter(dataset_display)\n",
        "display_images, display_labels = next(it)\n",
        "\n",
        "# predict pmf and labels for this dataset\n",
        "predict_labels_pmf = model.predict(x=dataset_display)\n",
        "predict_labels     = np.argmax(predict_labels_pmf, axis=1)\n",
        "\n",
        "# for display normalize images to [0, 1]\n",
        "display_images = ((display_images*DATA_STD_DEV.reshape((1, 1, 1, 3))) + DATA_MEAN.reshape((1, 1, 1, 3)))/255.0;\n",
        "\n",
        "# cycle through the images in the batch\n",
        "for image_index in range(predict_labels.size):\n",
        "    \n",
        "    # display the predicted label, actual label and image\n",
        "    print('Predicted label: {0:1d} and actual label: {1:1d}'.format(predict_labels[image_index], display_labels[image_index]))\n",
        "    plt.imshow(display_images[image_index, :, :, :])\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "   1563/Unknown - 112s 72ms/step - loss: 1.0556 - accuracy: 0.6282\n",
            "Epoch 00001: val_loss improved from 1.13455 to 1.05802, saving model to ./save/model/model_1.h5\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 1.0556 - accuracy: 0.6282 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0444 - accuracy: 0.6345\n",
            "Epoch 00002: val_loss did not improve from 1.05802\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 1.0446 - accuracy: 0.6345 - val_loss: 1.0659 - val_accuracy: 0.6264\n",
            "Epoch 3/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0604 - accuracy: 0.6274\n",
            "Epoch 00003: val_loss did not improve from 1.05802\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 1.0604 - accuracy: 0.6274 - val_loss: 1.0841 - val_accuracy: 0.6185\n",
            "Epoch 4/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0740 - accuracy: 0.6201\n",
            "Epoch 00004: val_loss did not improve from 1.05802\n",
            "1563/1563 [==============================] - 118s 76ms/step - loss: 1.0736 - accuracy: 0.6202 - val_loss: 1.1481 - val_accuracy: 0.6029\n",
            "Epoch 5/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0946 - accuracy: 0.6152\n",
            "Epoch 00005: val_loss did not improve from 1.05802\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 1.0946 - accuracy: 0.6152 - val_loss: 1.2257 - val_accuracy: 0.5673\n",
            "Epoch 6/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.1151 - accuracy: 0.6059\n",
            "Epoch 00006: val_loss did not improve from 1.05802\n",
            "1563/1563 [==============================] - 119s 76ms/step - loss: 1.1150 - accuracy: 0.6059 - val_loss: 1.2065 - val_accuracy: 0.5738\n",
            "Epoch 7/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.1140 - accuracy: 0.6066\n",
            "Epoch 00007: val_loss did not improve from 1.05802\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 1.1140 - accuracy: 0.6066 - val_loss: 1.1292 - val_accuracy: 0.5973\n",
            "Epoch 8/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.1047 - accuracy: 0.6083\n",
            "Epoch 00008: val_loss did not improve from 1.05802\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 1.1048 - accuracy: 0.6083 - val_loss: 1.1932 - val_accuracy: 0.5818\n",
            "Epoch 9/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.1054 - accuracy: 0.6099\n",
            "Epoch 00009: val_loss did not improve from 1.05802\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 1.1055 - accuracy: 0.6098 - val_loss: 1.1661 - val_accuracy: 0.5943\n",
            "Epoch 10/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.6108\n",
            "Epoch 00010: val_loss did not improve from 1.05802\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 1.0988 - accuracy: 0.6108 - val_loss: 1.1184 - val_accuracy: 0.6025\n",
            "Epoch 11/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0992 - accuracy: 0.6146\n",
            "Epoch 00011: val_loss did not improve from 1.05802\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 1.0991 - accuracy: 0.6146 - val_loss: 1.2753 - val_accuracy: 0.5583\n",
            "Epoch 12/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0927 - accuracy: 0.6138\n",
            "Epoch 00012: val_loss did not improve from 1.05802\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 1.0926 - accuracy: 0.6138 - val_loss: 1.1450 - val_accuracy: 0.5946\n",
            "Epoch 13/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0882 - accuracy: 0.6175\n",
            "Epoch 00013: val_loss did not improve from 1.05802\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 1.0879 - accuracy: 0.6175 - val_loss: 1.2116 - val_accuracy: 0.5816\n",
            "Epoch 14/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0831 - accuracy: 0.6164\n",
            "Epoch 00014: val_loss did not improve from 1.05802\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 1.0833 - accuracy: 0.6163 - val_loss: 1.1845 - val_accuracy: 0.5864\n",
            "Epoch 15/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0804 - accuracy: 0.6194\n",
            "Epoch 00015: val_loss did not improve from 1.05802\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 1.0806 - accuracy: 0.6194 - val_loss: 1.1315 - val_accuracy: 0.5994\n",
            "Epoch 16/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0771 - accuracy: 0.6216\n",
            "Epoch 00016: val_loss did not improve from 1.05802\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 1.0770 - accuracy: 0.6216 - val_loss: 1.1502 - val_accuracy: 0.5952\n",
            "Epoch 17/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0744 - accuracy: 0.6193\n",
            "Epoch 00017: val_loss did not improve from 1.05802\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 1.0743 - accuracy: 0.6193 - val_loss: 1.1846 - val_accuracy: 0.5829\n",
            "Epoch 18/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0683 - accuracy: 0.6248\n",
            "Epoch 00018: val_loss did not improve from 1.05802\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 1.0684 - accuracy: 0.6248 - val_loss: 1.1086 - val_accuracy: 0.6074\n",
            "Epoch 19/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0625 - accuracy: 0.6270\n",
            "Epoch 00019: val_loss did not improve from 1.05802\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 1.0626 - accuracy: 0.6269 - val_loss: 1.1330 - val_accuracy: 0.6012\n",
            "Epoch 20/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0579 - accuracy: 0.6272\n",
            "Epoch 00020: val_loss did not improve from 1.05802\n",
            "1563/1563 [==============================] - 116s 74ms/step - loss: 1.0579 - accuracy: 0.6273 - val_loss: 1.1603 - val_accuracy: 0.5882\n",
            "Epoch 21/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0532 - accuracy: 0.6283\n",
            "Epoch 00021: val_loss did not improve from 1.05802\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 1.0533 - accuracy: 0.6283 - val_loss: 1.0908 - val_accuracy: 0.6173\n",
            "Epoch 22/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0457 - accuracy: 0.6317\n",
            "Epoch 00022: val_loss did not improve from 1.05802\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 1.0456 - accuracy: 0.6317 - val_loss: 1.1055 - val_accuracy: 0.6106\n",
            "Epoch 23/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0454 - accuracy: 0.6319\n",
            "Epoch 00023: val_loss did not improve from 1.05802\n",
            "1563/1563 [==============================] - 116s 75ms/step - loss: 1.0452 - accuracy: 0.6319 - val_loss: 1.0823 - val_accuracy: 0.6256\n",
            "Epoch 24/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0355 - accuracy: 0.6341\n",
            "Epoch 00024: val_loss did not improve from 1.05802\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 1.0351 - accuracy: 0.6341 - val_loss: 1.1412 - val_accuracy: 0.6025\n",
            "Epoch 25/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0338 - accuracy: 0.6349\n",
            "Epoch 00025: val_loss did not improve from 1.05802\n",
            "1563/1563 [==============================] - 118s 76ms/step - loss: 1.0336 - accuracy: 0.6350 - val_loss: 1.0921 - val_accuracy: 0.6163\n",
            "Epoch 26/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0218 - accuracy: 0.6397\n",
            "Epoch 00026: val_loss did not improve from 1.05802\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 1.0214 - accuracy: 0.6398 - val_loss: 1.2077 - val_accuracy: 0.5923\n",
            "Epoch 27/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0237 - accuracy: 0.6403\n",
            "Epoch 00027: val_loss did not improve from 1.05802\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 1.0237 - accuracy: 0.6404 - val_loss: 1.0906 - val_accuracy: 0.6191\n",
            "Epoch 28/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0235 - accuracy: 0.6404\n",
            "Epoch 00028: val_loss improved from 1.05802 to 1.05385, saving model to ./save/model/model_28.h5\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 1.0232 - accuracy: 0.6404 - val_loss: 1.0539 - val_accuracy: 0.6336\n",
            "Epoch 29/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0140 - accuracy: 0.6430\n",
            "Epoch 00029: val_loss did not improve from 1.05385\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 1.0142 - accuracy: 0.6430 - val_loss: 1.0624 - val_accuracy: 0.6316\n",
            "Epoch 30/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0075 - accuracy: 0.6454\n",
            "Epoch 00030: val_loss did not improve from 1.05385\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 1.0078 - accuracy: 0.6454 - val_loss: 1.0549 - val_accuracy: 0.6329\n",
            "Epoch 31/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0048 - accuracy: 0.6469\n",
            "Epoch 00031: val_loss did not improve from 1.05385\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 1.0048 - accuracy: 0.6469 - val_loss: 1.0835 - val_accuracy: 0.6250\n",
            "Epoch 32/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0015 - accuracy: 0.6469\n",
            "Epoch 00032: val_loss did not improve from 1.05385\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 1.0017 - accuracy: 0.6468 - val_loss: 1.0651 - val_accuracy: 0.6292\n",
            "Epoch 33/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 1.0046 - accuracy: 0.6457\n",
            "Epoch 00033: val_loss did not improve from 1.05385\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 1.0044 - accuracy: 0.6457 - val_loss: 1.0665 - val_accuracy: 0.6268\n",
            "Epoch 34/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.9981 - accuracy: 0.6490\n",
            "Epoch 00034: val_loss improved from 1.05385 to 1.03715, saving model to ./save/model/model_34.h5\n",
            "1563/1563 [==============================] - 118s 75ms/step - loss: 0.9983 - accuracy: 0.6490 - val_loss: 1.0372 - val_accuracy: 0.6359\n",
            "Epoch 35/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.9860 - accuracy: 0.6540\n",
            "Epoch 00035: val_loss improved from 1.03715 to 1.03273, saving model to ./save/model/model_35.h5\n",
            "1563/1563 [==============================] - 117s 75ms/step - loss: 0.9856 - accuracy: 0.6541 - val_loss: 1.0327 - val_accuracy: 0.6395\n",
            "Epoch 36/60\n",
            "1562/1563 [============================>.] - ETA: 0s - loss: 0.9842 - accuracy: 0.6571\n",
            "Epoch 00036: val_loss did not improve from 1.03273\n",
            "1563/1563 [==============================] - 115s 73ms/step - loss: 0.9844 - accuracy: 0.6570 - val_loss: 1.0721 - val_accuracy: 0.6281\n",
            "Epoch 37/60\n",
            "1267/1563 [=======================>......] - ETA: 20s - loss: 0.9767 - accuracy: 0.6555Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}